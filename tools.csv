Category;Name;Description;Company;Payment;Link;arxiv;
Large Language Models (LLMs);Bloom;A multilingual LLM, many languages;BigScience;Open-source;https://huggingface.co/bigscience/bloom;
These are general-purpose language models that can be fine-tuned for document understanding tasks;Falcon;LLM for summarization, question answering, and more;Technology Innovation Institute;Open-source;https://huggingface.co/tiiuae/falcon-mamba-7b;
;Flan-T5;Instruction-tuned model based on T5, capable of zero-shot and fine-tuned document classification;Google;Open-source;https://huggingface.co/google/flan-t5-base;
;GPT-Neo and GPT-J;Open-source versions inspired by GPT-3, useful for NLP tasks;EleutherAI;Open-source;https://sapling.ai/llm/gpt-j-vs-gptneo;
;Kosmos-1;A rudimentary reimplementation of the KOSMOS-1 model described in Microsofts paper;;Open-source;https://github.com/bjoernpl/KOSMOS_reimplementation;
;LLaMA;Good for custom NLP tasks;Meta;Open-source;https://github.com/meta-llama/llama3;
;Mistral;Suitable for document processing applications;Mistral AI;Open-source;https://docs.mistral.ai/;
;Claude;Optimized for safe and interpretable responses;Anthropic;;https://docs.anthropic.com/en/home;
;Gemini;Audio, images, videos, and text;Google DeepMind;;https://ai.google.dev/gemini-api/docs;
;GPT-4;Generative Pre-trained Transformer;OpenAI;;https://platform.openai.com/docs/introduction;
;Grok;A variety of tasks, including generating and understanding text, code, and function calling;xAI;;https://x.ai/api;
Vision-Language Models (VLLMs);Donut;Designed for structured document understanding, works well with documents like invoices, receipts, and contracts;NAVER;Open-source;https://huggingface.co/naver-clova-ix/donut-base-finetuned-docvqa;
These models can understand and process both visual and textual data, useful for tasks like document layout understanding.;Molmo;A family of open vision-language models;;Open-source;https://huggingface.co/allenai/Molmo-7B-D-0924;
;NVLM;Open Frontier-Class Multimodal LLMs;;Open-source;https://nvlm-project.github.io/;
;Pix2Struct;Vision-language model, focused on document structure extraction, good for forms and charts;;Open-source;https://github.com/google-research/pix2struct;
;Qwen2-VL;SoTA understanding of images of various resolution & ratio;Allen Institute for AI;Open-source;https://github.com/QwenLM/Qwen2-VL;
;XLM-Roberta;Multilingual VLLM with fine-tuning potential for diverse text and document layouts;;Open-source;https://huggingface.co/laion/CLIP-ViT-B-32-xlm-roberta-base-laion5B-s13B-b90k;
;LayoutLMv3;Specialized in document layout understanding, with capabilities to recognize and classify text based on visual layout;Microsoft;;https://huggingface.co/docs/transformers/model_doc/layoutlmv3;
;LLaVA;Модель, которая объединяет языковые и визуальные способности для выполнения задач, связанных с пониманием изображений и текстов одновременно;Microsoft;;https://llava-vl.github.io/;
Multimodal Neural Networks;ALIGN;Can be used for image-text similarity and for zero-shot image classification;;Open-source;https://huggingface.co/docs/transformers/model_doc/align;
These models combine multiple modalities (text, vision, audio) for richer understanding and recognition.;BEiT-3;Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks;;Open-source;https://github.com/microsoft/unilm/tree/master/beit3;
;BLIP-2;A multimodal model tailored to image-to-text applications, supporting tasks like document captioning;Salesforce;Open-source;https://huggingface.co/docs/transformers/model_doc/blip-2;
;CLIP;A model trained on image-text pairs that can be applied to document image retrieval and classification;OpenAI;Open-source;https://github.com/openai/CLIP;
;CoCa;Модель может фокусироваться на различных аспектах мультимодальных данных, улучшая точность аннотаций и описаний;;Open-source;https://github.com/lucidrains/CoCa-pytorch;
;DocFormer;Применяется для понимания сложных документов и широко используется для задач, таких как классификация и информация;;Статья;https://github.com/shabie/docformer;https://arxiv.org/abs/2106.11539
;Flamingo;Multimodal model for tasks that require simultaneous understanding of images and text;;Статья;https://github.com/mlfoundations/open_flamingo;https://arxiv.org/abs/2204.14198
;FormNet;Performance on natural language and document understanding tasks;;Статья;;https://arxiv.org/abs/2203.08411
;LayoutLM;Обрабатывает текстовую информацию, визуальную информацию (размещение и форматирование текста), а также структуру документа, что позволяет извлекать данные из сложных многостраничных документов;Microsoft;Open-source;https://huggingface.co/docs/transformers/model_doc/layoutlm;
;MM1;Способна решать задачи, связанные с изображениями и текстом, например, подсчет объектов или выполнение математических операций, используя методы рассуждения;;Статья;;https://arxiv.org/abs/2403.09611
;OFA;A multimodal framework useful for various document-based visual and text tasks, like image-text retrieval;;Open-source;https://github.com/OFA-Sys/OFA;
;OmniFusion;Первая в России мультимодальная модель, способная поддерживать визуальный диалог и отвечать на вопросы по изображениям;AIRI;Open-source;https://github.com/AIRI-Institute/OmniFusion;
;PaLI;Генерация текстов на основе изображений и визуальный вопрос-ответ;;Open-source;https://github.com/kyegomez/PALI;https://arxiv.org/abs/2209.06794
;PandaGPT;Combines visual and language capabilities, enabling it to handle document tasks that require image-text interactions;Tencent AI Lab;Open-source;https://panda-gpt.github.io/;
;SimVLM;Используется для генерации описаний изображений и выполнения других задач мультимодального понимания;;Open-source;https://github.com/YulongBonjour/SimVLM;https://arxiv.org/abs/2108.10904
;StrucTexT;Structured text understanding on Visually Rich Documents (VRDs) is a crucial part of Document Intelligence;;Статья;;https://arxiv.org/abs/2108.02923
;TAPAS;For answering questions about tabular data;;Open-source;https://huggingface.co/docs/transformers/model_doc/tapas;
;TILT;Arctic-TILT is a Snowflake-grown LLM that leverages a proprietary and unique transformer architecture, tailored to understand and extract data from documents;Snowflake ;;https://www.snowflake.com/en/blog/arctic-tilt-compact-llm-advanced-document-ai/;
;UDOP;For document AI tasks like document image classification, document parsing and document visual question answering;;;https://huggingface.co/docs/transformers/model_doc/udop;
OCR Models for Document Recognition;Adobe OCR;OCR for PDFs, making it ideal for document workflows where PDFs need to be converted to editable or searchable formats;Adobe;;https://experienceleague.adobe.com/en/docs/document-cloud-learn/acrobat-learning/getting-started/scan-and-ocr;
These OCR engines are suitable for extracting text from structured and unstructured documents, including contracts, invoices, and receipts.;Amazon Textract;A cloud-based OCR service that goes beyond simple text extraction, capable of identifying structured data such as tables, forms, and key-value pairs;Amazon;;https://aws.amazon.com/textract/;
;CLaMM;Provides specialized OCR for historical and handwritten documents, useful for custom archival projects;;;https://clamm.irht.cnrs.fr/;
;DocTR;Robust two-stage OCR predictors that efficiently localize and recognize text in documents;;Open-source;https://github.com/mindee/doctr;
;EasyOCR;Built on PyTorch, it’s good for both handwritten and printed text across 80+ languages;;Open-source;https://github.com/JaidedAI/EasyOCR;
;Google Cloud Vision OCR;Part of its cloud-based image recognition services, supporting both printed and handwritten text in multiple languages;Google;;https://cloud.google.com/vision/docs/ocr;
;Keras-OCR;An OCR pipeline using Keras and TensorFlow, suitable for custom document OCR workflows;;Open-source;https://keras-ocr.readthedocs.io/en/latest/;
;M4C;Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA;;Open-source;https://mmf.sh/docs/projects/m4c/;
;Microsoft Azure OCR;Part of its cloud services, providing accurate text recognition in printed and handwritten text;Microsoft;;https://learn.microsoft.com/ru-ru/azure/ai-services/computer-vision/overview-ocr;
;PaddleOCR;Advanced OCR engine with multilingual support and high accuracy, great for document digitization tasks;PaddlePaddle (Baidu);Open-source;https://github.com/PaddlePaddle/PaddleOCR;
;SelfDoc;Использует подход самосупервизируемого обучения для обработки документов, извлекая информацию о структуре и содержании документа;;Статья;;https://arxiv.org/abs/2106.03331
;Tesseract OCR;A widely used, reliable OCR tool that supports multiple languages, ideal for printed text recognition;;Open-source;https://github.com/tesseract-ocr/tesseract;
;TrOCR;Transformer-based OCR model that can handle handwritten as well as printed text;Microsoft;Open-source;https://huggingface.co/docs/transformers/model_doc/trocr;
;ICDAR 2024 Proceedings;The 18th International Conference on Document Analysis and Recognition (ICDAR) features numerous papers on cutting-edge topics such as document image processing, layout analysis, and text recognition;;;https://icdar2024.net/procceedings/;
